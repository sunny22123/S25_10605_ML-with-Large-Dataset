{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGCUwTrwN_f2"
      },
      "source": [
        "# CMU auto-graded notebook\n",
        "\n",
        "Before you turn these assignments in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPEP_RA-N_f3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62fLXxvCtPSc"
      },
      "source": [
        "# Building Your Own Optimizers\n",
        "Optimizers tie the loss function and model parameters together by updating the model in response to the output of the loss function. In simpler terms, optimizers shape your model into its most accurate possible form by futzing with the weights. In even more simpler terms, optimizers define the way you want your model to be trained.\n",
        "\n",
        "The selection of optimizers could have dramatic effect on the performance of your model. If you choose a complex optimizer for a simple model, you would be likely to spend much more time in training with barely improved accuracy. And if you choose a naive optimizer for a complicated model, you would probably end up with poor accuracy.  \n",
        "\n",
        "This exercise will cover:\n",
        "- *Part 1: Calling existing optimizers provided by torch.optim*\n",
        "- *Part 2: Implementing and evaluating your own optimizers*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFFfr0xJymEU"
      },
      "source": [
        "## Part 1: Calling existing optimizers provided by torch.optim\n",
        "In this part, we will build up a demo pipeline, demonstrating how to involve optimizers given by `torch.optim` into the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK-Y1FCJzMAD"
      },
      "source": [
        "## (1a) Setting up the environment\n",
        "Just run the following cell to establish the runtime environment. Note that we're using PyTorch libraries for this part of assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EyUqv7tznpQ",
        "ExecuteTime": {
          "end_time": "2025-03-16T20:36:37.858091Z",
          "start_time": "2025-03-16T20:36:36.447236Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44a728f-3827-45bb-ef75-42a9795d6f74"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq1P1vvkz7J-"
      },
      "source": [
        "## (1b) Building a baseline model\n",
        "In this task, you will create a simple Linear Regression model with a synthetic toy dataset. Mean Squared Error (MSE) will be used as the loss metric.\n",
        "\n",
        "Note we have set a seed for `torch.manual_seed()` for auto grading purpose. Please don't change the seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "id": "YGJTEMT01sP0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e73462dde769e73dba5e6793e8220633",
          "grade": false,
          "grade_id": "LR_MSE",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# A toy dataset of points around y = 4 * x_1 + 3 * x_2 + 2\n",
        "NUM_EXAMPLES = 2000\n",
        "torch.manual_seed(10605)   # DON'T change this seed for reproducibility!\n",
        "\n",
        "X_train = torch.randn(NUM_EXAMPLES, 2)\n",
        "noise = torch.randn(NUM_EXAMPLES, 1)\n",
        "y_train = torch.matmul(X_train, torch.tensor([[4.], [3.]])) + 2 + noise\n",
        "\n",
        "# Definition of the Linear Regression model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.w = nn.Parameter(torch.tensor([[5.0, 5.0]]))  # Weight initialization\n",
        "        self.b = nn.Parameter(torch.tensor(10.0))          # Bias initialization\n",
        "\n",
        "    # Implementation of Linear Regression\n",
        "    # TODO: Replace <FILL IN> with appropriate code\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        This function will use current parameters to predict the labels for inputs.\n",
        "\n",
        "        Args:\n",
        "            inputs (torch.Tensor): The input tensor of this Linear Regression model.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted label\n",
        "        \"\"\"\n",
        "        return torch.matmul(inputs, self.w.T) + self.b\n",
        "\n",
        "\n",
        "# MSE loss function\n",
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "def loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate MSE loss between the predicted label and the actual label.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): The actual label.\n",
        "        y_pred (torch.Tensor): The predicted label.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: MSE loss value\n",
        "    \"\"\"\n",
        "    return torch.mean((y_true - y_pred) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QGIWie3vB05w",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f08b00caed24004bec31250b676ff240",
          "grade": true,
          "grade_id": "test_LR_MSE",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Test for LR_MSE\n",
        "model = LinearModel()\n",
        "# Sample input and output\n",
        "x = torch.tensor([[1.0, 2.0]])\n",
        "y = torch.tensor([3.0])\n",
        "\n",
        "assert np.equal(model(x).item(), 25.0), \"Wrong implementation of Linear Regression\"\n",
        "assert np.equal(loss(model(x), y).item(), 484.0), \"Wrong implementation of MSE loss\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7iwBiMKNW3"
      },
      "source": [
        "## (1c) Applying an existing optimizer\n",
        "Next, we will apply an Adam optimizer provided by `torch.optim` to our previously defined Linear Regression model. You can reuse this code snippet to validate your own optimizers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bDTTt3FtNEJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7351411b-8738-4f7c-fa8d-85abd37609ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 8.912812232971191\n",
            "Epoch 2/10, Loss: 1.2505282163619995\n",
            "Epoch 3/10, Loss: 0.9521957635879517\n",
            "Epoch 4/10, Loss: 0.9435355067253113\n",
            "Epoch 5/10, Loss: 0.9430028200149536\n",
            "Epoch 6/10, Loss: 0.9441074728965759\n",
            "Epoch 7/10, Loss: 0.9469836354255676\n",
            "Epoch 8/10, Loss: 0.9513348340988159\n",
            "Epoch 9/10, Loss: 0.9568836688995361\n",
            "Epoch 10/10, Loss: 0.9634056687355042\n"
          ]
        }
      ],
      "source": [
        "model = LinearModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        # Get mini-batch data\n",
        "        X_batch = X_train[i:i + batch_size]\n",
        "        y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        y_pred = model(X_batch)\n",
        "        current_loss = loss(y_batch, y_pred)\n",
        "\n",
        "        # backward\n",
        "        current_loss.backward()\n",
        "\n",
        "        # weight updates\n",
        "        optimizer.step()\n",
        "\n",
        "    # loss for the epoch\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {current_loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InSZvNu4RFFf"
      },
      "source": [
        "## Part 2: Implementing your own optimizers\n",
        "In this part, you will implement four different optimizers by yourself! Don't be panic if you're not good at math stuff, we will give you enough instructions and explanations!\n",
        "\n",
        "Generally, an optimizer class should contain three methods:\n",
        "- `__init__`: Initializes required parameters.\n",
        "-  `calculate_gradient`: Calculates gradients in the computational graph.\n",
        "- `apply_gradient`: Updates parameters of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yvaOmmUTyjz"
      },
      "source": [
        "## (2a) Gradient Descent\n",
        "Let's start with the simplest Gradient Descent.\n",
        "\n",
        "For a Linear Regression model with MSE where $\\mathbf{y}=w^T\\mathbf{X} + b$, the update rules of parameters at the $t$ th epoch are as follows:\n",
        "\n",
        "$$w^{(t+1)}=w^{(t)}-\\frac{\\alpha}{n} \\sum_{i=1}^{n} 2 \\cdot (h_{w,b}(\\mathbf{X}^{(i)}) - \\mathbf{y}^{(i)}) \\mathbf{X}^{(i)}$$\n",
        "\n",
        "$$b^{(t+1)}=b^{(t)}-\\frac{\\alpha}{n} \\sum_{i=1}^{n} 2 \\cdot (h_{w,b}(\\mathbf{X}^{(i)}) - \\mathbf{y}^{(i)}))$$\n",
        "\n",
        "where $n$ is the size of input dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "id": "Iv5EvSVQagfz",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9754bdc6190cf522677ec2e76afc24cb",
          "grade": false,
          "grade_id": "GD",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "class GD():\n",
        "    def __init__(self, lr=0.01):\n",
        "        \"\"\"Initializes required parameters.\n",
        "\n",
        "        Args:\n",
        "            lr (float): Learning rate.\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "\n",
        "    def calculate_gradient(self, X, y, model):\n",
        "        \"\"\"Calculates gradients manually for the linear regression model.\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): The input.\n",
        "            y (torch.Tensor): The actual label.\n",
        "            model: The model used to predict y with given input X.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: (gradient w.r.t w, gradient w.r.t b)\n",
        "        \"\"\"\n",
        "\n",
        "        n = X.shape[0]\n",
        "        y_pred = model.forward(X)\n",
        "\n",
        "        error = y_pred - y\n",
        "        grads_w = (2/n) * torch.matmul(X.T, error).T\n",
        "        grads_b = (2/n) * torch.sum(error)\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "\n",
        "    def apply_gradient(self, grads, model):\n",
        "        \"\"\"Updates parameters of the model.\n",
        "\n",
        "        Args:\n",
        "            grads (Tuple): (gradient w.r.t w, gradient w.r.t b).\n",
        "            model: The model used to predict y with given input X.\n",
        "        \"\"\"\n",
        "\n",
        "        grads_w, grads_b = grads\n",
        "        model.w -= self.lr * grads_w\n",
        "        model.b -= self.lr * grads_b\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FqAZAPWNsVRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c880ee3-fbb6-4e16-8d02-0e7e23a89bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 70.262\n",
            "Loss at epoch 0: 67.500\n",
            "Loss at epoch 20: 30.470\n",
            "Loss at epoch 40: 14.060\n",
            "Loss at epoch 60: 6.780\n",
            "Loss at epoch 80: 3.549\n",
            "Loss at epoch 100: 2.113\n",
            "Loss at epoch 120: 1.474\n",
            "Loss at epoch 140: 1.190\n",
            "Loss at epoch 160: 1.063\n",
            "Loss at epoch 180: 1.006\n",
            "Loss at epoch 200: 0.981\n",
            "Loss at epoch 220: 0.970\n",
            "Loss at epoch 240: 0.965\n",
            "Loss at epoch 260: 0.963\n",
            "Loss at epoch 280: 0.962\n",
            "Final loss: 0.961\n",
            "w = [[4.0558906 2.9847586]], b = 2.021142\n"
          ]
        }
      ],
      "source": [
        "class LinearModel:\n",
        "    def __init__(self):\n",
        "        self.w = torch.tensor([[5.0, 5.0]])  # Initial weights\n",
        "        self.b = torch.tensor(10.0)          # Initial bias\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X @ self.w.T + self.b\n",
        "\n",
        "# Evaluation\n",
        "model_0 = LinearModel()\n",
        "opt_0 = GD()\n",
        "\n",
        "print(\"Initial loss: %.3f\" % loss(model_0.forward(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  grads = opt_0.calculate_gradient(X_train, y_train, model_0)\n",
        "  opt_0.apply_gradient(grads, model_0)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_0.forward(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_0.forward(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_0.w.numpy(), model_0.b.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "qB8UveuntaAd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f29788020d6f3c633900f1c3b5174d46",
          "grade": true,
          "grade_id": "test_GD",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_0.forward(X_train), y_train), 0.9611, atol=1e-3), 'Wrong implementation of GD'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gpEJl78dWkW"
      },
      "source": [
        "## (2b) Stochastic Gradient Descent\n",
        "Let's take one step further. In SGD, you should randomly choose one data point to calculate the gradient each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "id": "U-uvm_TCdqTN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "111f3947040e52d9b536071859a9ee39",
          "grade": false,
          "grade_id": "SGD",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "class SGD():\n",
        "  def __init__(self, lr=0.01):\n",
        "    \"\"\"Initializes required parameters.\n",
        "\n",
        "      Args:\n",
        "          lr (float): Learning rate.\n",
        "    \"\"\"\n",
        "    self.lr = lr\n",
        "\n",
        "\n",
        "  def calculate_gradient(self, X, y, model):\n",
        "    \"\"\"Calculates gradients in the computational graph.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          X (torch.Tensor): The input.\n",
        "          y (torch.Tensor): The actual label.\n",
        "          model: The model used to predict y with given input X.\n",
        "\n",
        "      Returns:\n",
        "          Tuple: (gradient w.r.t w, gradient w.r.t b)\n",
        "    \"\"\"\n",
        "\n",
        "    # Randomly select a single data point (stochastic)\n",
        "    idx = torch.randint(0, X.shape[0], (1,))\n",
        "    X_sample = X[idx]\n",
        "    y_sample = y[idx]\n",
        "\n",
        "    # Get prediction for the sample\n",
        "    y_pred = model.forward(X_sample)\n",
        "\n",
        "    # Calculate the error\n",
        "    error = y_pred - y_sample\n",
        "\n",
        "    # Calculate gradients for w and b for this single point\n",
        "    # The 2 comes from derivative of squared error\n",
        "    grads_w = 2 * torch.matmul(error.T, X_sample)\n",
        "\n",
        "    # b grad: 2 * (y_pred - y)\n",
        "    grads_b = 2 * error.sum()\n",
        "\n",
        "    return (grads_w, grads_b)\n",
        "\n",
        "  def apply_gradient(self, grads, model):\n",
        "    \"\"\"Updates parameters of the model.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          grads (Tuple): (gradient w.r.t W, gradient w.r.t B).\n",
        "          model: The model used to predict y with given input X.\n",
        "    \"\"\"\n",
        "    # Extract gradients\n",
        "    grads_w, grads_b = grads\n",
        "\n",
        "    # Update parameters using gradient descent rule: param = param - lr * gradient\n",
        "    model.w -= self.lr * grads_w\n",
        "    model.b -= self.lr * grads_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5F5K9CzguDBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26bdd127-ef64-4fe1-81f7-4f3b29fed626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 70.262\n",
            "Loss at epoch 0: 66.670\n",
            "Loss at epoch 20: 34.439\n",
            "Loss at epoch 40: 13.381\n",
            "Loss at epoch 60: 7.430\n",
            "Loss at epoch 80: 3.402\n",
            "Loss at epoch 100: 2.068\n",
            "Loss at epoch 120: 1.561\n",
            "Loss at epoch 140: 1.308\n",
            "Loss at epoch 160: 1.057\n",
            "Loss at epoch 180: 0.976\n",
            "Loss at epoch 200: 1.027\n",
            "Loss at epoch 220: 1.025\n",
            "Loss at epoch 240: 1.035\n",
            "Loss at epoch 260: 1.052\n",
            "Loss at epoch 280: 0.998\n",
            "Final loss: 0.971\n",
            "w = [[4.113454  2.9197123]], b = 1.9429923\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "model_1 = LinearModel()\n",
        "opt_1 = SGD()\n",
        "\n",
        "print(\"Initial loss: %.3f\" % loss(model_1.forward(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  # Calculate current gradients\n",
        "  grads = opt_1.calculate_gradient(X_train, y_train, model_1)\n",
        "  # Use the optimizer to update gradients\n",
        "  opt_1.apply_gradient(grads, model_1)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_1.forward(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_1.forward(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_1.w.numpy(), model_1.b.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fYQkHSwGuOC_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3cfb94a77ae887117d88c6816d5cdc7d",
          "grade": true,
          "grade_id": "test_SGD",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_1.forward(X_train), y_train), 0.9707, atol=0.05), 'Wrong implementation of SGD'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIkQubmCuWWy"
      },
      "source": [
        "## (2c) AdaGrad\n",
        "\n",
        "One of the shortcomings about Gradient Descent is that its performance highly depends on the initial learning rate. Poor selection of initial learning rate would lead to either slow convergence or incapability of finding local minimum. In order to solve this problem, a revised version called AdaGrad is introduced.\n",
        "\n",
        "As its name suggests, AdaGrad makes its learning rate adaptive. To be more specific, AdaGrad uses past squared gradients to form an accumulated regularizer for its learning rate.\n",
        "\n",
        "Assuming $g^{(t)}$ is the gradient at $t$ th epoch, AdaGrad first calculates the accumulated squared gradients:\n",
        "\n",
        "$$r^{(t)}=r^{(t-1)}+(g^{(t)})^2$$\n",
        "\n",
        "Then AdaGrad will use this accumulated squared gradients to regularize its learning rate:\n",
        "\n",
        "$$w^{(t+1)}=w^{(t)}-\\frac{\\alpha}{\\sqrt{r^{(t)}} + \\epsilon} \\cdot g_w^{(t)}$$\n",
        "$$b^{(t+1)}=b^{(t)}-\\frac{\\alpha}{\\sqrt{r^{(t)}} + \\epsilon} \\cdot g_b^{(t)}$$\n",
        "\n",
        "where $\\epsilon$ is an additive constant (usually set as $10^{-7}$) that ensures we do not divide by 0.\n",
        "\n",
        "Note that you can reuse the `calculate_gradient` function in SGD for AdaGrad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "id": "dXcODnEjzrz1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e8b721afab643bf082a7ce14299296d8",
          "grade": false,
          "grade_id": "AdaGrad",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "class AdaGrad():\n",
        "  def __init__(self, lr=0.001, epsilon=1e-7):\n",
        "    \"\"\"Initializes required parameters.\n",
        "\n",
        "      Note: self.accumulator is the accumulated squared gradients. It is a tuple with following format:\n",
        "        (accumulator w.r.t w, accumulator w.r.t b)\n",
        "        And it should be initialized with the value of 0.1\n",
        "\n",
        "      Args:\n",
        "          lr (float): Learning rate.\n",
        "          epsilon (float): Additive constant that ensures we do not divide by 0.\n",
        "    \"\"\"\n",
        "    self.lr = lr\n",
        "    self.epsilon = epsilon\n",
        "    # Initialize accumulators for w and b with value 0.1\n",
        "    self.accumulator = (torch.tensor(0.1), torch.tensor(0.1))\n",
        "\n",
        "  def calculate_gradient(self, X, y, model):\n",
        "    \"\"\"Calculates gradients in the computational graph.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          X (torch.Tensor): The input.\n",
        "          y (torch.Tensor): The actual label.\n",
        "          model: The model used to predict y with given input X.\n",
        "\n",
        "      Returns:\n",
        "          Tuple: (gradient w.r.t w, gradient w.r.t b)\n",
        "    \"\"\"\n",
        "    # Randomly select a single data point (stochastic)\n",
        "    idx = torch.randint(0, X.shape[0], (1,))\n",
        "    X_sample = X[idx]\n",
        "    y_sample = y[idx]\n",
        "\n",
        "    # Get prediction for the sample\n",
        "    y_pred = model.forward(X_sample)\n",
        "\n",
        "    # Calculate the error\n",
        "    error = y_pred - y_sample\n",
        "\n",
        "    # Calculate gradients for w and b for this single point\n",
        "    # The 2 comes from derivative of squared error\n",
        "    grads_w = 2 * torch.matmul(error.T, X_sample)\n",
        "\n",
        "    # b grad: 2 * (y_pred - y)\n",
        "    grads_b = 2 * error.sum()\n",
        "\n",
        "    return (grads_w, grads_b)\n",
        "\n",
        "  def apply_gradient(self, grads, model):\n",
        "    \"\"\"Updates parameters of the model.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          grads (Tuple): (gradient w.r.t w, gradient w.r.t b).\n",
        "          model: The model used to predict y with given input X.\n",
        "    \"\"\"\n",
        "    # Extract gradients\n",
        "    grads_w, grads_b = grads\n",
        "\n",
        "    # Update accumulated squared gradients\n",
        "    acc_w, acc_b = self.accumulator\n",
        "    acc_w = acc_w + grads_w**2\n",
        "    acc_b = acc_b + grads_b**2\n",
        "    self.accumulator = (acc_w, acc_b)\n",
        "\n",
        "    # Update parameters using AdaGrad update rule\n",
        "    model.w -= self.lr * grads_w / (torch.sqrt(acc_w) + self.epsilon)\n",
        "    model.b -= self.lr * grads_b / (torch.sqrt(acc_b) + self.epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kzeYXHRZ2xZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7116b0c-0b19-4fba-ca97-589e2f40bf17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 70.262\n",
            "Loss at epoch 0: 68.047\n",
            "Loss at epoch 20: 59.313\n",
            "Loss at epoch 40: 54.384\n",
            "Loss at epoch 60: 50.898\n",
            "Loss at epoch 80: 48.182\n",
            "Loss at epoch 100: 45.781\n",
            "Loss at epoch 120: 43.249\n",
            "Loss at epoch 140: 41.087\n",
            "Loss at epoch 160: 39.465\n",
            "Loss at epoch 180: 37.913\n",
            "Loss at epoch 200: 36.551\n",
            "Loss at epoch 220: 35.223\n",
            "Loss at epoch 240: 33.439\n",
            "Loss at epoch 260: 32.117\n",
            "Loss at epoch 280: 30.790\n",
            "Final loss: 29.542\n",
            "w = [[4.658828  4.3051815]], b = 7.1320567\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "model_2 = LinearModel()\n",
        "opt_2 = AdaGrad(lr=0.1)\n",
        "\n",
        "print(\"Initial loss: %.3f\" % loss(model_2.forward(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  # Calculate current gradients\n",
        "  grads = opt_2.calculate_gradient(X_train, y_train, model_2)\n",
        "  # Use the optimizer to update gradients\n",
        "  opt_2.apply_gradient(grads, model_2)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_2.forward(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_2.forward(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_2.w.numpy(), model_2.b.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NhDsd7ge3DSh",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "51d3f5fa96af35d1ac41b8f88304daae",
          "grade": true,
          "grade_id": "test_AdaGrad",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_2.forward(X_train), y_train), 29.5416, atol=0.05), 'Wrong implementation of AdaGrad'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfBS1R4Y3Kzh"
      },
      "source": [
        "## (2d) Adam\n",
        "Although it sounds like a powerful algorithm, AdaGrad still has a bunch of shortcomings. One of them is that as the accumulated squared gradients becoming larger, the learning rate will be pushed to 0, leading to an early stop.\n",
        "\n",
        "Adam is designed to improve AdaGrad. It relies on adaptive moment estimation to set an independent adaptive learning rate for each parameter. The idea of Adam at the $t$ th epoch is described below:\n",
        "- Calculate current gradient $g^{(t)}$\n",
        "- Update biased first moment estimate\n",
        "  $$m^{(t)} = \\beta_1\\cdot m^{(t-1)}+(1-\\beta_1)\\cdot g^{(t)}$$\n",
        "- Update biased second raw moment estimate\n",
        "  $$v^{(t)} = \\beta_2\\cdot v^{(t-1)}+(1-\\beta_2)\\cdot (g^{(t)})^2$$\n",
        "- Compute bias-corrected first moment estimate\n",
        "  $$\\hat{m}^{(t)} = \\frac{m^{(t)}}{1-\\beta_1^t}$$\n",
        "- Compute bias-corrected second raw moment estimate\n",
        "  $$\\hat{v}^{(t)} = \\frac{v^{(t)}}{1-\\beta_2^t}$$\n",
        "- Update parameters with constrained learning rate\n",
        "  $$w^{(t)} = w^{(t-1)}-\\alpha\\cdot\\frac{\\hat{m}_w^{(t)}}{(\\sqrt{\\hat{v}_w^{(t)}}+\\epsilon)}$$\n",
        "  $$b^{(t)} = b^{(t-1)}-\\alpha\\cdot\\frac{\\hat{m}_b^{(t)}}{(\\sqrt{\\hat{v}_b^{(t)}}+\\epsilon)}$$\n",
        "\n",
        "where $\\beta_1$ and $\\beta_2$ are hyperparameters for the first and the second moment estimation\n",
        "\n",
        "As we can see from the update rule, Adam dynamically forms boundaries for its learning rate using first and second moment estimate without requiring too much memory space.\n",
        "\n",
        "Note that you can reuse the `calculate_gradient` function in SGD for Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "id": "FfEvvKMPBGIF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "203d4e2eb49f88a71afb37aa0b0f6208",
          "grade": false,
          "grade_id": "Adam",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# from tensorflow.tools.docs.doc_controls import T\n",
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "class Adam():\n",
        "  def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "    \"\"\"Initializes required parameters.\n",
        "\n",
        "      Note:\n",
        "        self.m is a list of Tensor, representing first moment estimations of gradients. The initial value should be [0].\n",
        "        self.v is a list of Tensor, representing second moment estimations of gradients. The initial value should be [0].\n",
        "        self.t is the timestep. The initial value should be 0.\n",
        "\n",
        "      Args:\n",
        "          lr (float): Learning rate.\n",
        "          epsilon (float): Additive constant that ensures we do not divide by 0.\n",
        "          beta1 (float): Hyperparameter for first moment estimation\n",
        "          beta2 (float): Hyperparameter for second moment estimation\n",
        "    \"\"\"\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "    self.m = [torch.tensor(0.0)]  # First moment estimate\n",
        "    self.v = [torch.tensor(0.0)]  # Second moment estimate\n",
        "    self.t = 0  # timestep counter\n",
        "\n",
        "\n",
        "  def calculate_gradient(self, X, y, model):\n",
        "    \"\"\"Calculates gradients in the computational graph.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          X (torch.Tensor): The input.\n",
        "          y (torch.Tensor): The actual label.\n",
        "          model: The model used to predict y with given input X.\n",
        "\n",
        "      Returns:\n",
        "          Tuple: (gradient w.r.t w, gradient w.r.t b)\n",
        "    \"\"\"\n",
        "    # Randomly select a single data point (stochastic)\n",
        "    idx = torch.randint(0, X.shape[0], (1,))\n",
        "    X_sample = X[idx]\n",
        "    y_sample = y[idx]\n",
        "\n",
        "    # Get prediction for the sample\n",
        "    y_pred = model.forward(X_sample)\n",
        "\n",
        "    # Calculate the error\n",
        "    error = y_pred - y_sample\n",
        "\n",
        "    # Calculate gradients for w and b for this single point\n",
        "    # The 2 comes from derivative of squared error\n",
        "    grads_w = 2 * torch.matmul(error.T, X_sample)\n",
        "\n",
        "    # b grad: 2 * (y_pred - y)\n",
        "    grads_b = 2 * error.sum()\n",
        "\n",
        "    return (grads_w, grads_b)\n",
        "\n",
        "  def apply_gradient(self, grads, model):\n",
        "    \"\"\"Updates parameters of the model.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          grads (Tuple): (gradient w.r.t w, gradient w.r.t b).\n",
        "          model: The model used to predict y with given input X.\n",
        "    \"\"\"\n",
        "    grads_w, grads_b = grads\n",
        "\n",
        "    self.t += 1\n",
        "\n",
        "    # If we only have one element in our lists, expand them\n",
        "    if len(self.m) == 1:\n",
        "        # Initialize for both w and b (m_w and m_b)\n",
        "        self.m = [torch.zeros_like(grads_w), torch.zeros_like(grads_b)]\n",
        "        # Initialize for both w and b (v_w and v_b)\n",
        "        self.v = [torch.zeros_like(grads_w), torch.zeros_like(grads_b)]\n",
        "\n",
        "    # Update biased first moment estimates\n",
        "    self.m[0] = self.beta1 * self.m[0] + (1 - self.beta1) * grads_w\n",
        "    self.m[1] = self.beta1 * self.m[1] + (1 - self.beta1) * grads_b\n",
        "\n",
        "    # Update biased second raw moment estimates\n",
        "    self.v[0] = self.beta2 * self.v[0] + (1 - self.beta2) * (grads_w ** 2)\n",
        "    self.v[1] = self.beta2 * self.v[1] + (1 - self.beta2) * (grads_b ** 2)\n",
        "\n",
        "    # Compute bias-corrected first moment estimates\n",
        "    m_hat_w = self.m[0] / (1 - self.beta1 ** self.t)\n",
        "    m_hat_b = self.m[1] / (1 - self.beta1 ** self.t)\n",
        "\n",
        "    # Compute bias-corrected second raw moment estimates\n",
        "    v_hat_w = self.v[0] / (1 - self.beta2 ** self.t)\n",
        "    v_hat_b = self.v[1] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "    # Update parameters\n",
        "    model.w -= self.lr * m_hat_w / (torch.sqrt(v_hat_w) + self.epsilon)\n",
        "    model.b -= self.lr * m_hat_b / (torch.sqrt(v_hat_b) + self.epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V0ImykFnDgGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705c3858-1cb7-48ea-e725-cee210964ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 70.262\n",
            "Loss at epoch 0: 68.542\n",
            "Loss at epoch 20: 39.030\n",
            "Loss at epoch 40: 20.803\n",
            "Loss at epoch 60: 10.552\n",
            "Loss at epoch 80: 4.869\n",
            "Loss at epoch 100: 2.466\n",
            "Loss at epoch 120: 1.443\n",
            "Loss at epoch 140: 1.065\n",
            "Loss at epoch 160: 0.999\n",
            "Loss at epoch 180: 1.020\n",
            "Loss at epoch 200: 1.047\n",
            "Loss at epoch 220: 1.106\n",
            "Loss at epoch 240: 1.139\n",
            "Loss at epoch 260: 0.983\n",
            "Loss at epoch 280: 0.963\n",
            "Final loss: 1.008\n",
            "w = [[4.1860504 3.128482 ]], b = 2.0925407\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "model_3 = LinearModel()\n",
        "opt_3 = Adam(lr=0.1)\n",
        "print(\"Initial loss: %.3f\" % loss(model_3.forward(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  # Calculate current gradients\n",
        "  grads = opt_3.calculate_gradient(X_train, y_train, model_3)\n",
        "  # Use the optimizer to update gradients\n",
        "  opt_3.apply_gradient(grads, model_3)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_3.forward(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_3.forward(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_3.w.numpy(), model_3.b.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "T4WougDyDswB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9b20bbff8cafa63968a8d947858b2f3",
          "grade": true,
          "grade_id": "test_Adam",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_3.forward(X_train), y_train), 1.0076, atol=0.05), 'Wrong implementation of Adam'\n",
        "assert np.allclose(model_3.w.numpy(), [[4.1860504, 3.1284819]], atol=0.05), 'Incorrect value of model.w'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}